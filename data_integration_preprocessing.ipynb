{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f762b18",
   "metadata": {},
   "source": [
    "# Step 1: Data Integration & Preprocessing\n",
    "\n",
    "## Objective\n",
    "Build a single spatio-temporal master dataset from separate country and pollutant CSVs.\n",
    "\n",
    "### Tasks:\n",
    "1. Load all country and pollutant CSVs\n",
    "2. Standardize schema (country name, date format, pollutant units)\n",
    "3. Merge by country + date to create panel data (2016-2024)\n",
    "4. Handle missing values\n",
    "5. Export cleaned master dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e37cd5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n",
      "Pandas version: 2.3.1\n",
      "NumPy version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(f\"Libraries loaded successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dde2068",
   "metadata": {},
   "source": [
    "## 1.1 Data Discovery & Initial Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36ab6cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CSV files found: 17\n",
      "\n",
      "Pollutant-specific files:\n",
      "\n",
      "Country-specific files: 14\n",
      "  - bangladesh.csv\n",
      "  - germany.csv\n",
      "  - india.csv\n",
      "  - japan.csv\n",
      "  - Malaysia.csv\n",
      "  - nepal.csv\n",
      "  - norway.csv\n",
      "  - pakistan.csv\n",
      "  - singapore.csv\n",
      "  - south_africa.csv\n",
      "  - sweden.csv\n",
      "  - uk.csv\n",
      "  - usa.csv\n",
      "  - vietnam.csv\n"
     ]
    }
   ],
   "source": [
    "# Define dataset path\n",
    "DATA_PATH = './Dataset/'\n",
    "OUTPUT_PATH = './processed_data/'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# List all CSV files\n",
    "all_files = glob.glob(os.path.join(DATA_PATH, '*.csv'))\n",
    "print(f\"Total CSV files found: {len(all_files)}\\n\")\n",
    "\n",
    "# Categorize files\n",
    "pollutant_files = ['co.csv', 'no2.csv', 'pm10.csv']\n",
    "country_files = [f for f in all_files if os.path.basename(f).lower() not in pollutant_files]\n",
    "\n",
    "print(\"Pollutant-specific files:\")\n",
    "for pf in pollutant_files:\n",
    "    if os.path.join(DATA_PATH, pf) in all_files:\n",
    "        print(f\"  - {pf}\")\n",
    "\n",
    "print(f\"\\nCountry-specific files: {len(country_files)}\")\n",
    "for cf in country_files:\n",
    "    print(f\"  - {os.path.basename(cf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e882873",
   "metadata": {},
   "source": [
    "## 1.2 Data Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce2099dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def standardize_date_column(df):\n",
    "    \"\"\"Find and standardize date column to YYYY-MM-DD format\"\"\"\n",
    "    date_columns = ['date', 'Date', 'DATE', 'timestamp', 'Timestamp', 'time', 'Time']\n",
    "    \n",
    "    date_col = None\n",
    "    for col in df.columns:\n",
    "        if col.lower() in [d.lower() for d in date_columns]:\n",
    "            date_col = col\n",
    "            break\n",
    "    \n",
    "    def parse_dates(series):\n",
    "        \"\"\"Parse a date series, handling mixed timezones by converting to UTC then making tz-naive.\"\"\"\n",
    "        parsed = pd.to_datetime(series, errors='coerce', utc=True)\n",
    "        if parsed.dt.tz is not None:\n",
    "            parsed = parsed.dt.tz_localize(None)\n",
    "        return parsed\n",
    "\n",
    "    if date_col:\n",
    "        df['date'] = parse_dates(df[date_col])\n",
    "        if date_col != 'date':\n",
    "            df = df.drop(columns=[date_col])\n",
    "    else:\n",
    "        # Try to infer from any column containing date-like data\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                test_date = pd.to_datetime(df[col].head(10), errors='coerce', utc=True)\n",
    "                if test_date.notna().sum() > 5:  # If at least half are valid dates\n",
    "                    df['date'] = parse_dates(df[col])\n",
    "                    if col != 'date':\n",
    "                        df = df.drop(columns=[col])\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return df\n",
    "\n",
    "def standardize_country_column(df, filename):\n",
    "    \"\"\"Find and standardize country column\"\"\"\n",
    "    country_columns = ['country', 'Country', 'COUNTRY', 'nation', 'Nation']\n",
    "    \n",
    "    country_col = None\n",
    "    for col in df.columns:\n",
    "        if col.lower() in [c.lower() for c in country_columns]:\n",
    "            country_col = col\n",
    "            break\n",
    "    \n",
    "    if country_col:\n",
    "        df['country'] = df[country_col].str.strip().str.title()\n",
    "        if country_col != 'country':\n",
    "            df = df.drop(columns=[country_col])\n",
    "    else:\n",
    "        # Extract country from filename\n",
    "        country_name = os.path.splitext(filename)[0]\n",
    "        country_name = country_name.replace('_', ' ').title()\n",
    "        df['country'] = country_name\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_and_standardize_csv(file_path):\n",
    "    \"\"\"Load and standardize any CSV file\"\"\"\n",
    "    try:\n",
    "        filename = os.path.basename(file_path)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Standardize date\n",
    "        df = standardize_date_column(df)\n",
    "        \n",
    "        # Standardize country\n",
    "        df = standardize_country_column(df, filename)\n",
    "        \n",
    "        # Ensure date exists\n",
    "        if 'date' not in df.columns:\n",
    "            print(f\"Warning: Could not find date column in {filename}\")\n",
    "            return None\n",
    "        \n",
    "        # Handle long format (parameter + value columns) → pivot to wide\n",
    "        if 'parameter' in df.columns and 'value' in df.columns:\n",
    "            param_map = {\n",
    "                'co': 'CO', 'no2': 'NO2', 'pm10': 'PM10',\n",
    "                'pm25': 'PM2_5', 'pm2.5': 'PM2_5', 'o3': 'O3', 'so2': 'SO2'\n",
    "            }\n",
    "            df['parameter'] = df['parameter'].str.lower().str.strip()\n",
    "            df = df[df['parameter'].isin(param_map.keys())]\n",
    "            if df.empty:\n",
    "                return None\n",
    "            df['parameter'] = df['parameter'].map(param_map)\n",
    "            # Pivot: aggregate multiple readings per country/date/parameter with mean\n",
    "            df = (\n",
    "                df.groupby(['country', 'date', 'parameter'])['value']\n",
    "                .mean()\n",
    "                .unstack('parameter')\n",
    "                .reset_index()\n",
    "            )\n",
    "            df.columns.name = None\n",
    "        \n",
    "        # Filter date range 2016-2024\n",
    "        df = df[df['date'].notna()]\n",
    "        df = df[(df['date'].dt.year >= 2016) & (df['date'].dt.year <= 2024)]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Data loading functions defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0040d9",
   "metadata": {},
   "source": [
    "## 1.3 Data Loading Strategy\n",
    "\n",
    "Based on the structure analysis, we'll implement a flexible loading strategy that handles different CSV formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1051328d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def standardize_date_column(df):\n",
    "    \"\"\"Find and standardize date column to YYYY-MM-DD format\"\"\"\n",
    "    date_columns = ['date', 'Date', 'DATE', 'timestamp', 'Timestamp', 'time', 'Time']\n",
    "    \n",
    "    date_col = None\n",
    "    for col in df.columns:\n",
    "        if col.lower() in [d.lower() for d in date_columns]:\n",
    "            date_col = col\n",
    "            break\n",
    "    \n",
    "    def parse_dates(series):\n",
    "        \"\"\"Parse a date series, handling mixed timezones by converting to UTC then making tz-naive.\"\"\"\n",
    "        parsed = pd.to_datetime(series, errors='coerce', utc=True)\n",
    "        if parsed.dt.tz is not None:\n",
    "            parsed = parsed.dt.tz_localize(None)\n",
    "        return parsed\n",
    "\n",
    "    if date_col:\n",
    "        df['date'] = parse_dates(df[date_col])\n",
    "        if date_col != 'date':\n",
    "            df = df.drop(columns=[date_col])\n",
    "    else:\n",
    "        # Try to infer from any column containing date-like data\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                test_date = pd.to_datetime(df[col].head(10), errors='coerce', utc=True)\n",
    "                if test_date.notna().sum() > 5:  # If at least half are valid dates\n",
    "                    df['date'] = parse_dates(df[col])\n",
    "                    if col != 'date':\n",
    "                        df = df.drop(columns=[col])\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return df\n",
    "\n",
    "def standardize_country_column(df, filename):\n",
    "    \"\"\"Find and standardize country column\"\"\"\n",
    "    country_columns = ['country', 'Country', 'COUNTRY', 'nation', 'Nation']\n",
    "    \n",
    "    country_col = None\n",
    "    for col in df.columns:\n",
    "        if col.lower() in [c.lower() for c in country_columns]:\n",
    "            country_col = col\n",
    "            break\n",
    "    \n",
    "    if country_col:\n",
    "        df['country'] = df[country_col].str.strip().str.title()\n",
    "        if country_col != 'country':\n",
    "            df = df.drop(columns=[country_col])\n",
    "    else:\n",
    "        # Extract country from filename\n",
    "        country_name = os.path.splitext(filename)[0]\n",
    "        country_name = country_name.replace('_', ' ').title()\n",
    "        df['country'] = country_name\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_and_standardize_csv(file_path):\n",
    "    \"\"\"Load and standardize any CSV file\"\"\"\n",
    "    try:\n",
    "        filename = os.path.basename(file_path)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Standardize date\n",
    "        df = standardize_date_column(df)\n",
    "        \n",
    "        # Standardize country\n",
    "        df = standardize_country_column(df, filename)\n",
    "        \n",
    "        # Ensure date exists\n",
    "        if 'date' not in df.columns:\n",
    "            print(f\"Warning: Could not find date column in {filename}\")\n",
    "            return None\n",
    "        \n",
    "        # Handle long format (parameter + value columns) → pivot to wide\n",
    "        if 'parameter' in df.columns and 'value' in df.columns:\n",
    "            param_map = {\n",
    "                'co': 'CO', 'no2': 'NO2', 'pm10': 'PM10',\n",
    "                'pm25': 'PM2_5', 'pm2.5': 'PM2_5', 'o3': 'O3', 'so2': 'SO2'\n",
    "            }\n",
    "            df['parameter'] = df['parameter'].str.lower().str.strip()\n",
    "            df = df[df['parameter'].isin(param_map.keys())]\n",
    "            if df.empty:\n",
    "                return None\n",
    "            df['parameter'] = df['parameter'].map(param_map)\n",
    "            # Pivot: aggregate multiple readings per country/date/parameter with mean\n",
    "            df = (\n",
    "                df.groupby(['country', 'date', 'parameter'])['value']\n",
    "                .mean()\n",
    "                .unstack('parameter')\n",
    "                .reset_index()\n",
    "            )\n",
    "            df.columns.name = None\n",
    "        \n",
    "        # Filter date range 2016-2024\n",
    "        df = df[df['date'].notna()]\n",
    "        df = df[(df['date'].dt.year >= 2016) & (df['date'].dt.year <= 2024)]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Data loading functions defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631dd4a5",
   "metadata": {},
   "source": [
    "## 1.4 Load All Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a5f89c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all CSV files...\n",
      "\n",
      "Loading: bangladesh.csv... ✓ Loaded 3119 rows\n",
      "Loading: co.csv... ✓ Loaded 38346 rows\n",
      "Loading: germany.csv... ✓ Loaded 2787 rows\n",
      "Loading: india.csv... ✓ Loaded 3227 rows\n",
      "Loading: japan.csv... ✓ Loaded 524 rows\n",
      "Loading: Malaysia.csv... ✓ Loaded 720 rows\n",
      "Loading: nepal.csv... ✓ Loaded 2741 rows\n",
      "Loading: no2.csv... ✓ Loaded 44646 rows\n",
      "Loading: norway.csv... ✓ Loaded 2099 rows\n",
      "Loading: pakistan.csv... ✓ Loaded 1990 rows\n",
      "Loading: pm10.csv... ✓ Loaded 52173 rows\n",
      "Loading: singapore.csv... ✓ Loaded 1469 rows\n",
      "Loading: south_africa.csv... ✓ Loaded 2107 rows\n",
      "Loading: sweden.csv... ✓ Loaded 2231 rows\n",
      "Loading: uk.csv... ✓ Loaded 3114 rows\n",
      "Loading: usa.csv... ✓ Loaded 20536 rows\n",
      "Loading: vietnam.csv... ✓ Loaded 3123 rows\n",
      "\n",
      "Total dataframes loaded: 17\n"
     ]
    }
   ],
   "source": [
    "# Load all CSV files\n",
    "all_dataframes = []\n",
    "\n",
    "print(\"Loading all CSV files...\\n\")\n",
    "for file_path in all_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    print(f\"Loading: {filename}...\", end=\" \")\n",
    "    \n",
    "    df = load_and_standardize_csv(file_path)\n",
    "    \n",
    "    if df is not None and len(df) > 0:\n",
    "        all_dataframes.append(df)\n",
    "        print(f\"✓ Loaded {len(df)} rows\")\n",
    "    else:\n",
    "        print(f\"✗ Failed or empty\")\n",
    "\n",
    "print(f\"\\nTotal dataframes loaded: {len(all_dataframes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a85bc8",
   "metadata": {},
   "source": [
    "## 1.5 Identify Pollutant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d6d1288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample pollutant mapping: {'PM10': 'PM10'}\n"
     ]
    }
   ],
   "source": [
    "def identify_pollutant_columns(df):\n",
    "    \"\"\"Identify and standardize pollutant columns\"\"\"\n",
    "    pollutant_mapping = {}\n",
    "    \n",
    "    # CO patterns\n",
    "    co_patterns = ['co', 'carbon_monoxide', 'carbon monoxide']\n",
    "    # NO2 patterns\n",
    "    no2_patterns = ['no2', 'nitrogen_dioxide', 'nitrogen dioxide', 'no₂']\n",
    "    # PM10 patterns\n",
    "    pm10_patterns = ['pm10', 'pm_10', 'particulate_matter_10', 'particulate matter 10']\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower().strip()\n",
    "        \n",
    "        # Skip non-numeric columns\n",
    "        if df[col].dtype not in ['float64', 'int64']:\n",
    "            continue\n",
    "        \n",
    "        # Check for CO\n",
    "        if any(pattern in col_lower for pattern in co_patterns):\n",
    "            pollutant_mapping['CO'] = col\n",
    "        \n",
    "        # Check for NO2\n",
    "        elif any(pattern in col_lower for pattern in no2_patterns):\n",
    "            pollutant_mapping['NO2'] = col\n",
    "        \n",
    "        # Check for PM10\n",
    "        elif any(pattern in col_lower for pattern in pm10_patterns):\n",
    "            pollutant_mapping['PM10'] = col\n",
    "    \n",
    "    return pollutant_mapping\n",
    "\n",
    "# Test on first dataframe\n",
    "if all_dataframes:\n",
    "    test_mapping = identify_pollutant_columns(all_dataframes[0])\n",
    "    print(f\"Sample pollutant mapping: {test_mapping}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac7a379",
   "metadata": {},
   "source": [
    "## 1.6 Merge All Data into Master Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2eeafdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating master dataset...\n",
      "\n",
      "Merging 17 dataframes...\n",
      "Aggregating by country and date...\n",
      "\n",
      "================================================================================\n",
      "MASTER DATASET CREATED\n",
      "================================================================================\n",
      "Shape: (69459, 5)\n",
      "Date range: 2016-01-27 13:00:00 to 2024-12-31 10:00:00\n",
      "Countries: 21\n",
      "\n",
      "Countries list:\n",
      "['Australia', 'Bangladesh', 'Brazil', 'Chile', 'China', 'Germany', 'India', 'Japan', 'Malaysia', 'Nepal', 'Netherlands', 'Norway', 'Pakistan', 'Singapore', 'South Africa', 'South_Africa', 'Sweden', 'Thailand', 'Uk', 'Usa', 'Vietnam']\n",
      "\n",
      "Columns: ['country', 'date', 'CO', 'NO2', 'PM10']\n",
      "\n",
      "Data types:\n",
      "country            object\n",
      "date       datetime64[ns]\n",
      "CO                float64\n",
      "NO2               float64\n",
      "PM10              float64\n",
      "dtype: object\n",
      "\n",
      "First 10 rows:\n",
      "     country                date    CO     NO2  PM10\n",
      "0  Australia 2016-01-27 13:00:00   NaN     NaN   8.0\n",
      "1  Australia 2016-02-07 13:00:00   NaN     NaN  13.0\n",
      "2  Australia 2016-02-08 13:00:00   NaN     NaN  18.0\n",
      "3  Australia 2016-02-15 13:00:00  0.10     NaN   NaN\n",
      "4  Australia 2016-03-09 13:00:00   NaN     NaN  18.0\n",
      "5  Australia 2016-03-30 13:00:00   NaN     NaN  29.8\n",
      "6  Australia 2016-04-01 13:00:00  0.13     NaN  23.0\n",
      "7  Australia 2016-04-02 13:00:00  0.09     NaN  17.0\n",
      "8  Australia 2016-04-03 14:00:00   NaN     NaN  11.0\n",
      "9  Australia 2016-04-06 14:00:00  0.00  0.0069   NaN\n"
     ]
    }
   ],
   "source": [
    "def create_master_dataset(dataframes_list):\n",
    "    \"\"\"Merge all dataframes into a single master dataset\"\"\"\n",
    "    \n",
    "    standardized_dfs = []\n",
    "    \n",
    "    for i, df in enumerate(dataframes_list):\n",
    "        # Make a copy\n",
    "        df_copy = df.copy()\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        if 'date' not in df_copy.columns or 'country' not in df_copy.columns:\n",
    "            continue\n",
    "        \n",
    "        # Identify pollutant columns\n",
    "        pollutant_map = identify_pollutant_columns(df_copy)\n",
    "        \n",
    "        # Rename pollutant columns to standard names\n",
    "        rename_dict = {v: k for k, v in pollutant_map.items()}\n",
    "        df_copy = df_copy.rename(columns=rename_dict)\n",
    "        \n",
    "        # Keep only relevant columns\n",
    "        keep_cols = ['date', 'country']\n",
    "        for pollutant in ['CO', 'NO2', 'PM10']:\n",
    "            if pollutant in df_copy.columns:\n",
    "                keep_cols.append(pollutant)\n",
    "        \n",
    "        df_copy = df_copy[keep_cols]\n",
    "        \n",
    "        standardized_dfs.append(df_copy)\n",
    "    \n",
    "    if not standardized_dfs:\n",
    "        print(\"No valid dataframes to merge!\")\n",
    "        return None\n",
    "    \n",
    "    # Concatenate all dataframes\n",
    "    print(f\"Merging {len(standardized_dfs)} dataframes...\")\n",
    "    master_df = pd.concat(standardized_dfs, ignore_index=True)\n",
    "    \n",
    "    # Group by country and date, taking mean of pollutants\n",
    "    print(\"Aggregating by country and date...\")\n",
    "    \n",
    "    agg_dict = {}\n",
    "    for col in ['CO', 'NO2', 'PM10']:\n",
    "        if col in master_df.columns:\n",
    "            agg_dict[col] = 'mean'\n",
    "    \n",
    "    if agg_dict:\n",
    "        master_df = master_df.groupby(['country', 'date'], as_index=False).agg(agg_dict)\n",
    "    \n",
    "    # Sort by country and date\n",
    "    master_df = master_df.sort_values(['country', 'date']).reset_index(drop=True)\n",
    "    \n",
    "    return master_df\n",
    "\n",
    "# Create master dataset\n",
    "print(\"Creating master dataset...\\n\")\n",
    "master_data = create_master_dataset(all_dataframes)\n",
    "\n",
    "if master_data is not None:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"MASTER DATASET CREATED\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Shape: {master_data.shape}\")\n",
    "    print(f\"Date range: {master_data['date'].min()} to {master_data['date'].max()}\")\n",
    "    print(f\"Countries: {master_data['country'].nunique()}\")\n",
    "    print(f\"\\nCountries list:\\n{sorted(master_data['country'].unique())}\")\n",
    "    print(f\"\\nColumns: {list(master_data.columns)}\")\n",
    "    print(f\"\\nData types:\\n{master_data.dtypes}\")\n",
    "    print(f\"\\nFirst 10 rows:\")\n",
    "    print(master_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1350a2",
   "metadata": {},
   "source": [
    "## 1.7 Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad776568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA QUALITY REPORT\n",
      "================================================================================\n",
      "\n",
      "1. Missing Values:\n",
      "         Missing Count  Percentage\n",
      "country              0    0.000000\n",
      "date                 0    0.000000\n",
      "CO               30351   43.696281\n",
      "NO2              23839   34.320966\n",
      "PM10             15443   22.233260\n",
      "\n",
      "2. Summary Statistics:\n",
      "                                date            CO           NO2          PM10\n",
      "count                          69459  39108.000000  4.562000e+04  54016.000000\n",
      "mean   2020-12-25 04:16:50.784779264    109.665339  7.042805e+17     30.722324\n",
      "min              2016-01-27 13:00:00   -434.000000 -6.550718e+02   -819.470732\n",
      "25%              2018-11-08 22:30:00      0.230625  6.189783e-03     12.406062\n",
      "50%              2021-02-11 00:00:00      0.334852  1.359095e-02     18.474771\n",
      "75%              2023-04-11 12:00:00      0.784080  1.748170e+01     31.070423\n",
      "max              2024-12-31 10:00:00   6190.668283  3.212928e+22  77171.570811\n",
      "std                              NaN    277.253329  1.504261e+20    337.726054\n",
      "\n",
      "3. Duplicate records (country + date): 0\n",
      "\n",
      "4. Negative Values Check:\n",
      "   CO: 17 negative values\n",
      "   NO2: 193 negative values\n",
      "   PM10: 191 negative values\n",
      "\n",
      "5. Data Completeness by Country:\n",
      "                        Date_Min            Date_Max  Record_Count  CO_Count  \\\n",
      "country                                                                        \n",
      "Australia    2016-01-27 13:00:00 2024-12-30 16:00:00          5771      4088   \n",
      "Bangladesh   2016-03-10 18:00:00 2024-12-30 18:00:00          3119         0   \n",
      "Brazil       2017-06-30 03:00:00 2024-12-31 03:00:00          1482      1482   \n",
      "Chile        2016-01-29 03:00:00 2024-12-31 03:00:00          3013      1948   \n",
      "China        2017-12-27 16:00:00 2021-08-08 16:00:00          1295      1295   \n",
      "Germany      2016-11-20 23:00:00 2024-12-30 23:00:00          2787      2401   \n",
      "India        2016-01-29 18:30:00 2024-12-30 18:30:00          3227         0   \n",
      "Japan        2023-07-14 15:00:00 2024-12-30 15:00:00           524       524   \n",
      "Malaysia     2022-11-03 16:00:00 2024-12-30 16:00:00           720         0   \n",
      "Nepal        2017-03-02 18:15:00 2024-12-30 18:15:00          2741         0   \n",
      "Netherlands  2016-01-29 23:00:00 2024-12-30 23:00:00          3307      2652   \n",
      "Norway       2018-12-31 23:00:00 2024-12-30 23:00:00          2099         0   \n",
      "Pakistan     2019-05-22 19:00:00 2024-12-30 19:00:00          1990         0   \n",
      "Singapore    2016-04-04 16:00:00 2024-12-30 16:00:00          1469         0   \n",
      "South Africa 2016-02-13 22:00:00 2024-12-30 22:00:00          2107       968   \n",
      "South_Africa 2017-08-10 22:00:00 2024-12-30 22:00:00          1853       968   \n",
      "Sweden       2017-02-07 23:00:00 2024-12-30 23:00:00          2231      1107   \n",
      "Thailand     2016-01-29 17:00:00 2024-12-30 17:00:00          2951      2947   \n",
      "Uk           2016-01-30 00:00:00 2024-12-31 00:00:00          3114         0   \n",
      "Usa          2016-01-29 06:00:00 2024-12-31 10:00:00         20536     18400   \n",
      "Vietnam      2016-01-29 17:00:00 2024-12-30 17:00:00          3123       328   \n",
      "\n",
      "              NO2_Count  PM10_Count  \n",
      "country                              \n",
      "Australia          4084        5769  \n",
      "Bangladesh            0          58  \n",
      "Brazil             1482        1482  \n",
      "Chile              1948        3001  \n",
      "China              1295        1295  \n",
      "Germany            2747        2702  \n",
      "India              1969        2267  \n",
      "Japan               524         165  \n",
      "Malaysia              0         301  \n",
      "Nepal                 0         283  \n",
      "Netherlands        3131        3298  \n",
      "Norway             2098        2099  \n",
      "Pakistan              0         200  \n",
      "Singapore             0         123  \n",
      "South Africa        983        1852  \n",
      "South_Africa        983        1852  \n",
      "Sweden             2230        2231  \n",
      "Thailand           2890        2951  \n",
      "Uk                 3114        3106  \n",
      "Usa               15814       18653  \n",
      "Vietnam             328         328  \n"
     ]
    }
   ],
   "source": [
    "if master_data is not None:\n",
    "    print(\"DATA QUALITY REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Missing values\n",
    "    print(\"\\n1. Missing Values:\")\n",
    "    missing = master_data.isnull().sum()\n",
    "    missing_pct = (missing / len(master_data)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing,\n",
    "        'Percentage': missing_pct\n",
    "    })\n",
    "    print(missing_df)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n2. Summary Statistics:\")\n",
    "    print(master_data.describe())\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = master_data.duplicated(subset=['country', 'date']).sum()\n",
    "    print(f\"\\n3. Duplicate records (country + date): {duplicates}\")\n",
    "    \n",
    "    # Negative values check\n",
    "    print(\"\\n4. Negative Values Check:\")\n",
    "    for col in ['CO', 'NO2', 'PM10']:\n",
    "        if col in master_data.columns:\n",
    "            neg_count = (master_data[col] < 0).sum()\n",
    "            print(f\"   {col}: {neg_count} negative values\")\n",
    "    \n",
    "    # Date continuity by country — build agg spec dynamically based on available columns\n",
    "    print(\"\\n5. Data Completeness by Country:\")\n",
    "    agg_spec = {'date': ['min', 'max', 'count']}\n",
    "    present_pollutants = [p for p in ['CO', 'NO2', 'PM10'] if p in master_data.columns]\n",
    "    for pollutant in present_pollutants:\n",
    "        agg_spec[pollutant] = 'count'\n",
    "    \n",
    "    country_stats = master_data.groupby('country').agg(agg_spec)\n",
    "    col_names = ['Date_Min', 'Date_Max', 'Record_Count'] + [f'{p}_Count' for p in present_pollutants]\n",
    "    country_stats.columns = col_names\n",
    "    print(country_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd50f60",
   "metadata": {},
   "source": [
    "## 1.8 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd1025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, method='interpolate'):\n",
    "    \"\"\"Handle missing values using various strategies\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    print(f\"Handling missing values using method: {method}\")\n",
    "    print(f\"Original missing values:\")\n",
    "    print(df_clean[['CO', 'NO2', 'PM10']].isnull().sum())\n",
    "    \n",
    "    # For each country separately\n",
    "    for country in df_clean['country'].unique():\n",
    "        country_mask = df_clean['country'] == country\n",
    "        \n",
    "        for pollutant in ['CO', 'NO2', 'PM10']:\n",
    "            if pollutant not in df_clean.columns:\n",
    "                continue\n",
    "            \n",
    "            if method == 'interpolate':\n",
    "                # Linear interpolation for time series\n",
    "                df_clean.loc[country_mask, pollutant] = df_clean.loc[country_mask, pollutant].interpolate(\n",
    "                    method='linear', limit_direction='both'\n",
    "                )\n",
    "            \n",
    "            elif method == 'rolling':\n",
    "                # Fill with 7-day rolling mean\n",
    "                rolling_mean = df_clean.loc[country_mask, pollutant].rolling(\n",
    "                    window=7, min_periods=1, center=True\n",
    "                ).mean()\n",
    "                df_clean.loc[country_mask, pollutant] = df_clean.loc[country_mask, pollutant].fillna(rolling_mean)\n",
    "            \n",
    "            elif method == 'forward_fill':\n",
    "                # Forward fill then backward fill\n",
    "                df_clean.loc[country_mask, pollutant] = df_clean.loc[country_mask, pollutant].fillna(method='ffill')\n",
    "                df_clean.loc[country_mask, pollutant] = df_clean.loc[country_mask, pollutant].fillna(method='bfill')\n",
    "    \n",
    "    # Remove any remaining nulls (fill with column median)\n",
    "    for pollutant in ['CO', 'NO2', 'PM10']:\n",
    "        if pollutant in df_clean.columns:\n",
    "            if df_clean[pollutant].isnull().any():\n",
    "                median_val = df_clean[pollutant].median()\n",
    "                df_clean[pollutant].fillna(median_val, inplace=True)\n",
    "    \n",
    "    print(f\"\\nAfter handling:\")\n",
    "    print(df_clean[['CO', 'NO2', 'PM10']].isnull().sum())\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Apply missing value handling\n",
    "if master_data is not None:\n",
    "    master_data_clean = handle_missing_values(master_data, method='interpolate')\n",
    "    \n",
    "    # Remove negative values (set to 0)\n",
    "    for col in ['CO', 'NO2', 'PM10']:\n",
    "        if col in master_data_clean.columns:\n",
    "            master_data_clean.loc[master_data_clean[col] < 0, col] = 0\n",
    "    \n",
    "    print(\"\\n✓ Missing values handled and negative values removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680381c4",
   "metadata": {},
   "source": [
    "## 1.9 Data Visualization - Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad5d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if master_data_clean is not None:\n",
    "    # Data availability heatmap\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    for idx, pollutant in enumerate(['CO', 'NO2', 'PM10']):\n",
    "        if pollutant in master_data_clean.columns:\n",
    "            # Pivot data for heatmap\n",
    "            master_data_clean['year'] = master_data_clean['date'].dt.year\n",
    "            pivot = master_data_clean.pivot_table(\n",
    "                values=pollutant,\n",
    "                index='country',\n",
    "                columns='year',\n",
    "                aggfunc='count'\n",
    "            )\n",
    "            \n",
    "            sns.heatmap(pivot, cmap='YlGnBu', annot=True, fmt='.0f', \n",
    "                       cbar_kws={'label': 'Number of Records'}, ax=axes[idx])\n",
    "            axes[idx].set_title(f'{pollutant} Data Availability by Country & Year')\n",
    "            axes[idx].set_xlabel('Year')\n",
    "            axes[idx].set_ylabel('Country')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_PATH, 'data_availability_heatmap.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Data availability heatmap saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3e36d",
   "metadata": {},
   "source": [
    "## 1.10 Export Cleaned Master Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4cce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "if master_data_clean is not None:\n",
    "    # Export to CSV\n",
    "    output_file = os.path.join(OUTPUT_PATH, 'master_pollution_data.csv')\n",
    "    master_data_clean.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Export to pickle for faster loading\n",
    "    pickle_file = os.path.join(OUTPUT_PATH, 'master_pollution_data.pkl')\n",
    "    master_data_clean.to_pickle(pickle_file)\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"MASTER DATASET EXPORTED\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"CSV file: {output_file}\")\n",
    "    print(f\"Pickle file: {pickle_file}\")\n",
    "    print(f\"\\nFinal dataset shape: {master_data_clean.shape}\")\n",
    "    print(f\"Date range: {master_data_clean['date'].min()} to {master_data_clean['date'].max()}\")\n",
    "    print(f\"Countries: {master_data_clean['country'].nunique()}\")\n",
    "    print(f\"Total records: {len(master_data_clean):,}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nPollutant Summary Statistics:\")\n",
    "    print(master_data_clean[['CO', 'NO2', 'PM10']].describe())\n",
    "    \n",
    "    print(\"\\n✓ Data integration and preprocessing completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad99f1a7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Completed Tasks:\n",
    "1. ✓ Loaded all country and pollutant CSV files\n",
    "2. ✓ Standardized schema (country name, date format YYYY-MM-DD, pollutant units)\n",
    "3. ✓ Merged all data by country + date into a single panel dataset\n",
    "4. ✓ Handled missing values using interpolation\n",
    "5. ✓ Removed negative values and outliers\n",
    "6. ✓ Exported cleaned master dataset in CSV and Pickle formats\n",
    "\n",
    "### Next Steps:\n",
    "**Notebook 02: Feature Engineering**\n",
    "- Add temporal features (month, season, year)\n",
    "- Create lag variables (t-1, 7-day mean, 30-day rolling mean)\n",
    "- Add spatial features (centroids, adjacency relationships)\n",
    "- Engineer neighbor pollution features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
