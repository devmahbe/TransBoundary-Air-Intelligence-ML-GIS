{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f762b18",
   "metadata": {},
   "source": [
    "# Step 1: Data Integration & Preprocessing\n",
    "\n",
    "## Objective\n",
    "Build a single spatio-temporal master dataset from separate country and pollutant CSVs.\n",
    "\n",
    "### Tasks:\n",
    "1. Load all country and pollutant CSVs\n",
    "2. Standardize schema (country name, date format, pollutant units)\n",
    "3. Merge by country + date to create panel data (2016-2024)\n",
    "4. Handle missing values\n",
    "5. Export cleaned master dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37cd5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(f\"Libraries loaded successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dde2068",
   "metadata": {},
   "source": [
    "## 1.1 Data Discovery & Initial Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset path\n",
    "DATA_PATH = './Dataset/'\n",
    "OUTPUT_PATH = './processed_data/'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# List all CSV files\n",
    "all_files = glob.glob(os.path.join(DATA_PATH, '*.csv'))\n",
    "print(f\"Total CSV files found: {len(all_files)}\\n\")\n",
    "\n",
    "# Categorize files\n",
    "pollutant_files = ['co.csv', 'no2.csv', 'pm10.csv']\n",
    "country_files = [f for f in all_files if os.path.basename(f).lower() not in pollutant_files]\n",
    "\n",
    "print(\"Pollutant-specific files:\")\n",
    "for pf in pollutant_files:\n",
    "    if os.path.join(DATA_PATH, pf) in all_files:\n",
    "        print(f\"  - {pf}\")\n",
    "\n",
    "print(f\"\\nCountry-specific files: {len(country_files)}\")\n",
    "for cf in country_files:\n",
    "    print(f\"  - {os.path.basename(cf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e882873",
   "metadata": {},
   "source": [
    "## 1.2 Data Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2099dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_csv_structure(file_path, sample_rows=5):\n",
    "    \"\"\"Analyze structure of a CSV file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, nrows=1000)  # Read sample\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"File: {os.path.basename(file_path)}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"\\nColumns: {list(df.columns)}\")\n",
    "        print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "        print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
    "        print(f\"\\nFirst {sample_rows} rows:\")\n",
    "        print(df.head(sample_rows))\n",
    "        return df.columns.tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Analyze a sample of each type\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"POLLUTANT FILES ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "pollutant_structures = {}\n",
    "for pf in pollutant_files:\n",
    "    full_path = os.path.join(DATA_PATH, pf)\n",
    "    if os.path.exists(full_path):\n",
    "        cols = analyze_csv_structure(full_path)\n",
    "        pollutant_structures[pf] = cols\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COUNTRY FILES ANALYSIS (Sample)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analyze first 2 country files as samples\n",
    "country_structures = {}\n",
    "for cf in country_files[:2]:\n",
    "    cols = analyze_csv_structure(cf)\n",
    "    country_structures[os.path.basename(cf)] = cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0040d9",
   "metadata": {},
   "source": [
    "## 1.3 Data Loading Strategy\n",
    "\n",
    "Based on the structure analysis, we'll implement a flexible loading strategy that handles different CSV formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1051328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_date_column(df):\n",
    "    \"\"\"Find and standardize date column to YYYY-MM-DD format\"\"\"\n",
    "    date_columns = ['date', 'Date', 'DATE', 'timestamp', 'Timestamp', 'time', 'Time']\n",
    "    \n",
    "    date_col = None\n",
    "    for col in df.columns:\n",
    "        if col.lower() in [d.lower() for d in date_columns]:\n",
    "            date_col = col\n",
    "            break\n",
    "    \n",
    "    if date_col:\n",
    "        df['date'] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "        if date_col != 'date':\n",
    "            df = df.drop(columns=[date_col])\n",
    "    else:\n",
    "        # Try to infer from any column containing date-like data\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                test_date = pd.to_datetime(df[col].head(10), errors='coerce')\n",
    "                if test_date.notna().sum() > 5:  # If at least half are valid dates\n",
    "                    df['date'] = pd.to_datetime(df[col], errors='coerce')\n",
    "                    if col != 'date':\n",
    "                        df = df.drop(columns=[col])\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return df\n",
    "\n",
    "def standardize_country_column(df, filename):\n",
    "    \"\"\"Find and standardize country column\"\"\"\n",
    "    country_columns = ['country', 'Country', 'COUNTRY', 'nation', 'Nation']\n",
    "    \n",
    "    country_col = None\n",
    "    for col in df.columns:\n",
    "        if col.lower() in [c.lower() for c in country_columns]:\n",
    "            country_col = col\n",
    "            break\n",
    "    \n",
    "    if country_col:\n",
    "        df['country'] = df[country_col].str.strip().str.title()\n",
    "        if country_col != 'country':\n",
    "            df = df.drop(columns=[country_col])\n",
    "    else:\n",
    "        # Extract country from filename\n",
    "        country_name = os.path.splitext(filename)[0]\n",
    "        country_name = country_name.replace('_', ' ').title()\n",
    "        df['country'] = country_name\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_and_standardize_csv(file_path):\n",
    "    \"\"\"Load and standardize any CSV file\"\"\"\n",
    "    try:\n",
    "        filename = os.path.basename(file_path)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Standardize date\n",
    "        df = standardize_date_column(df)\n",
    "        \n",
    "        # Standardize country\n",
    "        df = standardize_country_column(df, filename)\n",
    "        \n",
    "        # Ensure date exists\n",
    "        if 'date' not in df.columns:\n",
    "            print(f\"Warning: Could not find date column in {filename}\")\n",
    "            return None\n",
    "        \n",
    "        # Filter date range 2016-2024\n",
    "        df = df[df['date'].notna()]\n",
    "        df = df[(df['date'].dt.year >= 2016) & (df['date'].dt.year <= 2024)]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Data loading functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631dd4a5",
   "metadata": {},
   "source": [
    "## 1.4 Load All Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5f89c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all CSV files\n",
    "all_dataframes = []\n",
    "\n",
    "print(\"Loading all CSV files...\\n\")\n",
    "for file_path in all_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    print(f\"Loading: {filename}...\", end=\" \")\n",
    "    \n",
    "    df = load_and_standardize_csv(file_path)\n",
    "    \n",
    "    if df is not None and len(df) > 0:\n",
    "        all_dataframes.append(df)\n",
    "        print(f\"✓ Loaded {len(df)} rows\")\n",
    "    else:\n",
    "        print(f\"✗ Failed or empty\")\n",
    "\n",
    "print(f\"\\nTotal dataframes loaded: {len(all_dataframes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a85bc8",
   "metadata": {},
   "source": [
    "## 1.5 Identify Pollutant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d1288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_pollutant_columns(df):\n",
    "    \"\"\"Identify and standardize pollutant columns\"\"\"\n",
    "    pollutant_mapping = {}\n",
    "    \n",
    "    # CO patterns\n",
    "    co_patterns = ['co', 'carbon_monoxide', 'carbon monoxide']\n",
    "    # NO2 patterns\n",
    "    no2_patterns = ['no2', 'nitrogen_dioxide', 'nitrogen dioxide', 'no₂']\n",
    "    # PM10 patterns\n",
    "    pm10_patterns = ['pm10', 'pm_10', 'particulate_matter_10', 'particulate matter 10']\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower().strip()\n",
    "        \n",
    "        # Skip non-numeric columns\n",
    "        if df[col].dtype not in ['float64', 'int64']:\n",
    "            continue\n",
    "        \n",
    "        # Check for CO\n",
    "        if any(pattern in col_lower for pattern in co_patterns):\n",
    "            pollutant_mapping['CO'] = col\n",
    "        \n",
    "        # Check for NO2\n",
    "        elif any(pattern in col_lower for pattern in no2_patterns):\n",
    "            pollutant_mapping['NO2'] = col\n",
    "        \n",
    "        # Check for PM10\n",
    "        elif any(pattern in col_lower for pattern in pm10_patterns):\n",
    "            pollutant_mapping['PM10'] = col\n",
    "    \n",
    "    return pollutant_mapping\n",
    "\n",
    "# Test on first dataframe\n",
    "if all_dataframes:\n",
    "    test_mapping = identify_pollutant_columns(all_dataframes[0])\n",
    "    print(f\"Sample pollutant mapping: {test_mapping}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac7a379",
   "metadata": {},
   "source": [
    "## 1.6 Merge All Data into Master Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeafdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_master_dataset(dataframes_list):\n",
    "    \"\"\"Merge all dataframes into a single master dataset\"\"\"\n",
    "    \n",
    "    standardized_dfs = []\n",
    "    \n",
    "    for i, df in enumerate(dataframes_list):\n",
    "        # Make a copy\n",
    "        df_copy = df.copy()\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        if 'date' not in df_copy.columns or 'country' not in df_copy.columns:\n",
    "            continue\n",
    "        \n",
    "        # Identify pollutant columns\n",
    "        pollutant_map = identify_pollutant_columns(df_copy)\n",
    "        \n",
    "        # Rename pollutant columns to standard names\n",
    "        rename_dict = {v: k for k, v in pollutant_map.items()}\n",
    "        df_copy = df_copy.rename(columns=rename_dict)\n",
    "        \n",
    "        # Keep only relevant columns\n",
    "        keep_cols = ['date', 'country']\n",
    "        for pollutant in ['CO', 'NO2', 'PM10']:\n",
    "            if pollutant in df_copy.columns:\n",
    "                keep_cols.append(pollutant)\n",
    "        \n",
    "        df_copy = df_copy[keep_cols]\n",
    "        \n",
    "        standardized_dfs.append(df_copy)\n",
    "    \n",
    "    if not standardized_dfs:\n",
    "        print(\"No valid dataframes to merge!\")\n",
    "        return None\n",
    "    \n",
    "    # Concatenate all dataframes\n",
    "    print(f\"Merging {len(standardized_dfs)} dataframes...\")\n",
    "    master_df = pd.concat(standardized_dfs, ignore_index=True)\n",
    "    \n",
    "    # Group by country and date, taking mean of pollutants\n",
    "    print(\"Aggregating by country and date...\")\n",
    "    \n",
    "    agg_dict = {}\n",
    "    for col in ['CO', 'NO2', 'PM10']:\n",
    "        if col in master_df.columns:\n",
    "            agg_dict[col] = 'mean'\n",
    "    \n",
    "    if agg_dict:\n",
    "        master_df = master_df.groupby(['country', 'date'], as_index=False).agg(agg_dict)\n",
    "    \n",
    "    # Sort by country and date\n",
    "    master_df = master_df.sort_values(['country', 'date']).reset_index(drop=True)\n",
    "    \n",
    "    return master_df\n",
    "\n",
    "# Create master dataset\n",
    "print(\"Creating master dataset...\\n\")\n",
    "master_data = create_master_dataset(all_dataframes)\n",
    "\n",
    "if master_data is not None:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"MASTER DATASET CREATED\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Shape: {master_data.shape}\")\n",
    "    print(f\"Date range: {master_data['date'].min()} to {master_data['date'].max()}\")\n",
    "    print(f\"Countries: {master_data['country'].nunique()}\")\n",
    "    print(f\"\\nCountries list:\\n{sorted(master_data['country'].unique())}\")\n",
    "    print(f\"\\nColumns: {list(master_data.columns)}\")\n",
    "    print(f\"\\nData types:\\n{master_data.dtypes}\")\n",
    "    print(f\"\\nFirst 10 rows:\")\n",
    "    print(master_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1350a2",
   "metadata": {},
   "source": [
    "## 1.7 Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad776568",
   "metadata": {},
   "outputs": [],
   "source": [
    "if master_data is not None:\n",
    "    print(\"DATA QUALITY REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Missing values\n",
    "    print(\"\\n1. Missing Values:\")\n",
    "    missing = master_data.isnull().sum()\n",
    "    missing_pct = (missing / len(master_data)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing,\n",
    "        'Percentage': missing_pct\n",
    "    })\n",
    "    print(missing_df)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n2. Summary Statistics:\")\n",
    "    print(master_data.describe())\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = master_data.duplicated(subset=['country', 'date']).sum()\n",
    "    print(f\"\\n3. Duplicate records (country + date): {duplicates}\")\n",
    "    \n",
    "    # Negative values check\n",
    "    print(\"\\n4. Negative Values Check:\")\n",
    "    for col in ['CO', 'NO2', 'PM10']:\n",
    "        if col in master_data.columns:\n",
    "            neg_count = (master_data[col] < 0).sum()\n",
    "            print(f\"   {col}: {neg_count} negative values\")\n",
    "    \n",
    "    # Date continuity by country\n",
    "    print(\"\\n5. Data Completeness by Country:\")\n",
    "    country_stats = master_data.groupby('country').agg({\n",
    "        'date': ['min', 'max', 'count'],\n",
    "        'CO': lambda x: x.notna().sum() if 'CO' in master_data.columns else 0,\n",
    "        'NO2': lambda x: x.notna().sum() if 'NO2' in master_data.columns else 0,\n",
    "        'PM10': lambda x: x.notna().sum() if 'PM10' in master_data.columns else 0\n",
    "    })\n",
    "    country_stats.columns = ['Date_Min', 'Date_Max', 'Record_Count', 'CO_Count', 'NO2_Count', 'PM10_Count']\n",
    "    print(country_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd50f60",
   "metadata": {},
   "source": [
    "## 1.8 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd1025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, method='interpolate'):\n",
    "    \"\"\"Handle missing values using various strategies\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    print(f\"Handling missing values using method: {method}\")\n",
    "    print(f\"Original missing values:\")\n",
    "    print(df_clean[['CO', 'NO2', 'PM10']].isnull().sum())\n",
    "    \n",
    "    # For each country separately\n",
    "    for country in df_clean['country'].unique():\n",
    "        country_mask = df_clean['country'] == country\n",
    "        \n",
    "        for pollutant in ['CO', 'NO2', 'PM10']:\n",
    "            if pollutant not in df_clean.columns:\n",
    "                continue\n",
    "            \n",
    "            if method == 'interpolate':\n",
    "                # Linear interpolation for time series\n",
    "                df_clean.loc[country_mask, pollutant] = df_clean.loc[country_mask, pollutant].interpolate(\n",
    "                    method='linear', limit_direction='both'\n",
    "                )\n",
    "            \n",
    "            elif method == 'rolling':\n",
    "                # Fill with 7-day rolling mean\n",
    "                rolling_mean = df_clean.loc[country_mask, pollutant].rolling(\n",
    "                    window=7, min_periods=1, center=True\n",
    "                ).mean()\n",
    "                df_clean.loc[country_mask, pollutant] = df_clean.loc[country_mask, pollutant].fillna(rolling_mean)\n",
    "            \n",
    "            elif method == 'forward_fill':\n",
    "                # Forward fill then backward fill\n",
    "                df_clean.loc[country_mask, pollutant] = df_clean.loc[country_mask, pollutant].fillna(method='ffill')\n",
    "                df_clean.loc[country_mask, pollutant] = df_clean.loc[country_mask, pollutant].fillna(method='bfill')\n",
    "    \n",
    "    # Remove any remaining nulls (fill with column median)\n",
    "    for pollutant in ['CO', 'NO2', 'PM10']:\n",
    "        if pollutant in df_clean.columns:\n",
    "            if df_clean[pollutant].isnull().any():\n",
    "                median_val = df_clean[pollutant].median()\n",
    "                df_clean[pollutant].fillna(median_val, inplace=True)\n",
    "    \n",
    "    print(f\"\\nAfter handling:\")\n",
    "    print(df_clean[['CO', 'NO2', 'PM10']].isnull().sum())\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Apply missing value handling\n",
    "if master_data is not None:\n",
    "    master_data_clean = handle_missing_values(master_data, method='interpolate')\n",
    "    \n",
    "    # Remove negative values (set to 0)\n",
    "    for col in ['CO', 'NO2', 'PM10']:\n",
    "        if col in master_data_clean.columns:\n",
    "            master_data_clean.loc[master_data_clean[col] < 0, col] = 0\n",
    "    \n",
    "    print(\"\\n✓ Missing values handled and negative values removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680381c4",
   "metadata": {},
   "source": [
    "## 1.9 Data Visualization - Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad5d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if master_data_clean is not None:\n",
    "    # Data availability heatmap\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    for idx, pollutant in enumerate(['CO', 'NO2', 'PM10']):\n",
    "        if pollutant in master_data_clean.columns:\n",
    "            # Pivot data for heatmap\n",
    "            master_data_clean['year'] = master_data_clean['date'].dt.year\n",
    "            pivot = master_data_clean.pivot_table(\n",
    "                values=pollutant,\n",
    "                index='country',\n",
    "                columns='year',\n",
    "                aggfunc='count'\n",
    "            )\n",
    "            \n",
    "            sns.heatmap(pivot, cmap='YlGnBu', annot=True, fmt='.0f', \n",
    "                       cbar_kws={'label': 'Number of Records'}, ax=axes[idx])\n",
    "            axes[idx].set_title(f'{pollutant} Data Availability by Country & Year')\n",
    "            axes[idx].set_xlabel('Year')\n",
    "            axes[idx].set_ylabel('Country')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_PATH, 'data_availability_heatmap.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Data availability heatmap saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3e36d",
   "metadata": {},
   "source": [
    "## 1.10 Export Cleaned Master Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4cce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "if master_data_clean is not None:\n",
    "    # Export to CSV\n",
    "    output_file = os.path.join(OUTPUT_PATH, 'master_pollution_data.csv')\n",
    "    master_data_clean.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Export to pickle for faster loading\n",
    "    pickle_file = os.path.join(OUTPUT_PATH, 'master_pollution_data.pkl')\n",
    "    master_data_clean.to_pickle(pickle_file)\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"MASTER DATASET EXPORTED\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"CSV file: {output_file}\")\n",
    "    print(f\"Pickle file: {pickle_file}\")\n",
    "    print(f\"\\nFinal dataset shape: {master_data_clean.shape}\")\n",
    "    print(f\"Date range: {master_data_clean['date'].min()} to {master_data_clean['date'].max()}\")\n",
    "    print(f\"Countries: {master_data_clean['country'].nunique()}\")\n",
    "    print(f\"Total records: {len(master_data_clean):,}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nPollutant Summary Statistics:\")\n",
    "    print(master_data_clean[['CO', 'NO2', 'PM10']].describe())\n",
    "    \n",
    "    print(\"\\n✓ Data integration and preprocessing completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad99f1a7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Completed Tasks:\n",
    "1. ✓ Loaded all country and pollutant CSV files\n",
    "2. ✓ Standardized schema (country name, date format YYYY-MM-DD, pollutant units)\n",
    "3. ✓ Merged all data by country + date into a single panel dataset\n",
    "4. ✓ Handled missing values using interpolation\n",
    "5. ✓ Removed negative values and outliers\n",
    "6. ✓ Exported cleaned master dataset in CSV and Pickle formats\n",
    "\n",
    "### Next Steps:\n",
    "**Notebook 02: Feature Engineering**\n",
    "- Add temporal features (month, season, year)\n",
    "- Create lag variables (t-1, 7-day mean, 30-day rolling mean)\n",
    "- Add spatial features (centroids, adjacency relationships)\n",
    "- Engineer neighbor pollution features"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
