{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d999022",
   "metadata": {},
   "source": [
    "# Step 5: Explainable AI - SHAP Analysis\n",
    "\n",
    "## Objective\n",
    "Apply SHAP (SHapley Additive exPlanations) to understand model predictions and quantify transboundary pollution influence.\n",
    "\n",
    "### Tasks:\n",
    "1. Apply SHAP analysis to best models\n",
    "2. Extract global feature importance\n",
    "3. Analyze individual predictions\n",
    "4. Quantify country-to-country influence\n",
    "5. Create influence strength matrices\n",
    "6. Generate policy-relevant insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8af780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# SHAP for explainability\n",
    "import shap\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n",
    "print(f\"SHAP version: {shap.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a8555d",
   "metadata": {},
   "source": [
    "## 5.1 Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c96ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_PATH = './processed_data/'\n",
    "MODEL_PATH = './model_outputs/'\n",
    "OUTPUT_PATH = './shap_outputs/'\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data and models...\")\n",
    "data = pd.read_pickle(os.path.join(DATA_PATH, 'features_engineered_with_country.pkl'))\n",
    "\n",
    "# Load feature info\n",
    "with open(os.path.join(DATA_PATH, 'feature_info.json'), 'r') as f:\n",
    "    feature_info = json.load(f)\n",
    "\n",
    "# Load model performance\n",
    "results_df = pd.read_csv(os.path.join(MODEL_PATH, 'model_performance.csv'))\n",
    "\n",
    "# Load predictions\n",
    "predictions_df = pd.read_pickle(os.path.join(MODEL_PATH, 'test_predictions.pkl'))\n",
    "\n",
    "TARGET_POLLUTANTS = ['CO', 'NO2', 'PM10']\n",
    "EXCLUDE_COLS = ['country', 'date', 'season'] + TARGET_POLLUTANTS\n",
    "feature_columns = [col for col in data.columns if col not in EXCLUDE_COLS]\n",
    "\n",
    "# Load best models\n",
    "best_models = {}\n",
    "for pollutant in TARGET_POLLUTANTS:\n",
    "    model_file = os.path.join(MODEL_PATH, f'best_model_{pollutant}.pkl')\n",
    "    best_models[pollutant] = joblib.load(model_file)\n",
    "    model_name = results_df[results_df['Pollutant'] == pollutant].sort_values('R²', ascending=False).iloc[0]['Model']\n",
    "    print(f\"✓ Loaded {pollutant} model: {model_name}\")\n",
    "\n",
    "print(f\"\\nData shape: {data.shape}\")\n",
    "print(f\"Features: {len(feature_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca69b9",
   "metadata": {},
   "source": [
    "## 5.2 Prepare Test Data for SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d12f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and split data (same as training)\n",
    "data_clean = data.copy()\n",
    "data_clean = data_clean.replace([np.inf, -np.inf], np.nan)\n",
    "data_clean = data_clean.fillna(0)\n",
    "data_clean = data_clean.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Use same split as training (80/20)\n",
    "split_idx = int(len(data_clean) * 0.8)\n",
    "test_data = data_clean.iloc[split_idx:].copy()\n",
    "\n",
    "# Prepare feature matrix\n",
    "X_test = test_data[feature_columns]\n",
    "\n",
    "# Sample for SHAP (SHAP can be slow on large datasets)\n",
    "# Use stratified sampling by country\n",
    "sample_size = min(1000, len(test_data))\n",
    "X_test_sample = test_data.groupby('country', group_keys=False).apply(\n",
    "    lambda x: x.sample(min(len(x), sample_size // test_data['country'].nunique()))\n",
    ").sample(sample_size)\n",
    "\n",
    "X_shap = X_test_sample[feature_columns]\n",
    "test_sample_data = X_test_sample.copy()\n",
    "\n",
    "print(f\"Test set size: {len(test_data):,}\")\n",
    "print(f\"SHAP sample size: {len(X_shap):,}\")\n",
    "print(f\"Countries in sample: {X_test_sample['country'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aee780",
   "metadata": {},
   "source": [
    "## 5.3 SHAP Analysis for Each Pollutant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d99955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP explainers and values storage\n",
    "shap_explainers = {}\n",
    "shap_values_dict = {}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPUTING SHAP VALUES\")\n",
    "print(\"=\"*80)\n",
    "print(\"This may take several minutes...\\n\")\n",
    "\n",
    "for pollutant in TARGET_POLLUTANTS:\n",
    "    print(f\"\\nProcessing {pollutant}...\")\n",
    "    \n",
    "    model = best_models[pollutant]\n",
    "    \n",
    "    try:\n",
    "        # Create SHAP explainer (TreeExplainer for tree-based models)\n",
    "        print(f\"  Creating explainer...\")\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        \n",
    "        # Calculate SHAP values\n",
    "        print(f\"  Calculating SHAP values...\")\n",
    "        shap_values = explainer.shap_values(X_shap)\n",
    "        \n",
    "        # Store results\n",
    "        shap_explainers[pollutant] = explainer\n",
    "        shap_values_dict[pollutant] = shap_values\n",
    "        \n",
    "        print(f\"  ✓ SHAP values computed for {pollutant}\")\n",
    "        print(f\"    Shape: {shap_values.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error computing SHAP for {pollutant}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ SHAP COMPUTATION COMPLETED\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b87d290",
   "metadata": {},
   "source": [
    "## 5.4 Global Feature Importance (SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8afecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary plots (global importance)\n",
    "for pollutant in TARGET_POLLUTANTS:\n",
    "    if pollutant not in shap_values_dict:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nCreating SHAP summary plot for {pollutant}...\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    shap.summary_plot(\n",
    "        shap_values_dict[pollutant], \n",
    "        X_shap, \n",
    "        max_display=20,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f'SHAP Feature Importance - {pollutant}', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_PATH, f'shap_summary_{pollutant}.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"  ✓ Summary plot saved\")\n",
    "\n",
    "print(\"\\n✓ Global SHAP importance plots created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c249ac4",
   "metadata": {},
   "source": [
    "## 5.5 Mean Absolute SHAP Values (Feature Ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fde30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute SHAP values\n",
    "shap_importance = {}\n",
    "\n",
    "for pollutant in TARGET_POLLUTANTS:\n",
    "    if pollutant not in shap_values_dict:\n",
    "        continue\n",
    "    \n",
    "    # Mean absolute SHAP value for each feature\n",
    "    mean_abs_shap = np.abs(shap_values_dict[pollutant]).mean(axis=0)\n",
    "    \n",
    "    shap_df = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'mean_abs_shap': mean_abs_shap\n",
    "    }).sort_values('mean_abs_shap', ascending=False)\n",
    "    \n",
    "    shap_importance[pollutant] = shap_df\n",
    "    \n",
    "    # Save to CSV\n",
    "    shap_df.to_csv(os.path.join(OUTPUT_PATH, f'shap_importance_{pollutant}.csv'), index=False)\n",
    "    \n",
    "    print(f\"\\nTop 15 features for {pollutant} (by SHAP importance):\")\n",
    "    print(shap_df.head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\n✓ SHAP importance rankings saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6274b5",
   "metadata": {},
   "source": [
    "## 5.6 SHAP Bar Plots (Top Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1fceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plots for top features\n",
    "for pollutant in TARGET_POLLUTANTS:\n",
    "    if pollutant not in shap_values_dict:\n",
    "        continue\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(\n",
    "        shap_values_dict[pollutant], \n",
    "        X_shap, \n",
    "        plot_type=\"bar\",\n",
    "        max_display=20,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f'Top 20 Features (Mean |SHAP|) - {pollutant}', fontsize=14, fontweight='bold', pad=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_PATH, f'shap_bar_{pollutant}.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ SHAP bar plots created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b771ad2d",
   "metadata": {},
   "source": [
    "## 5.7 Neighbor Influence Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12bcada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract neighbor feature importance\n",
    "neighbor_influence = {}\n",
    "\n",
    "for pollutant in TARGET_POLLUTANTS:\n",
    "    if pollutant not in shap_importance:\n",
    "        continue\n",
    "    \n",
    "    shap_df = shap_importance[pollutant]\n",
    "    \n",
    "    # Filter neighbor features\n",
    "    neighbor_features = shap_df[shap_df['feature'].str.contains('neighbor')].copy()\n",
    "    \n",
    "    # Self features (lag, rolling)\n",
    "    self_features = shap_df[\n",
    "        (shap_df['feature'].str.contains(pollutant)) & \n",
    "        (~shap_df['feature'].str.contains('neighbor'))\n",
    "    ].copy()\n",
    "    \n",
    "    # Calculate total influence\n",
    "    neighbor_total = neighbor_features['mean_abs_shap'].sum()\n",
    "    self_total = self_features['mean_abs_shap'].sum()\n",
    "    \n",
    "    neighbor_influence[pollutant] = {\n",
    "        'neighbor_importance': neighbor_total,\n",
    "        'self_importance': self_total,\n",
    "        'neighbor_percentage': (neighbor_total / (neighbor_total + self_total)) * 100,\n",
    "        'self_percentage': (self_total / (neighbor_total + self_total)) * 100\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{pollutant} Influence Analysis:\")\n",
    "    print(f\"  Neighbor influence: {neighbor_total:.4f} ({neighbor_influence[pollutant]['neighbor_percentage']:.2f}%)\")\n",
    "    print(f\"  Self influence: {self_total:.4f} ({neighbor_influence[pollutant]['self_percentage']:.2f}%)\")\n",
    "    print(f\"\\n  Top 5 neighbor features:\")\n",
    "    print(neighbor_features.head(5).to_string(index=False))\n",
    "\n",
    "# Save influence summary\n",
    "influence_df = pd.DataFrame(neighbor_influence).T\n",
    "influence_df.to_csv(os.path.join(OUTPUT_PATH, 'neighbor_vs_self_influence.csv'))\n",
    "\n",
    "print(\"\\n✓ Neighbor influence quantified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e82a181",
   "metadata": {},
   "source": [
    "## 5.8 Visualize Self vs Neighbor Influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccb73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of self vs neighbor influence\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart\n",
    "pollutants = list(neighbor_influence.keys())\n",
    "neighbor_pcts = [neighbor_influence[p]['neighbor_percentage'] for p in pollutants]\n",
    "self_pcts = [neighbor_influence[p]['self_percentage'] for p in pollutants]\n",
    "\n",
    "x = np.arange(len(pollutants))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, neighbor_pcts, width, label='Neighbor Influence', color='#FF6B6B', alpha=0.8)\n",
    "ax1.bar(x + width/2, self_pcts, width, label='Self Influence', color='#4ECDC4', alpha=0.8)\n",
    "ax1.set_xlabel('Pollutant', fontsize=12)\n",
    "ax1.set_ylabel('Influence (%)', fontsize=12)\n",
    "ax1.set_title('Self vs Neighbor Influence (by SHAP)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(pollutants)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Pie chart for average\n",
    "avg_neighbor = np.mean(neighbor_pcts)\n",
    "avg_self = np.mean(self_pcts)\n",
    "\n",
    "ax2.pie([avg_neighbor, avg_self], \n",
    "        labels=['Neighbor Influence', 'Self Influence'],\n",
    "        colors=['#FF6B6B', '#4ECDC4'],\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        textprops={'fontsize': 12})\n",
    "ax2.set_title('Average Influence Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'self_vs_neighbor_influence.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Self vs neighbor influence visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b04710",
   "metadata": {},
   "source": [
    "## 5.9 SHAP Dependence Plots (Key Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba0500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependence plots for top 3 features of each pollutant\n",
    "for pollutant in TARGET_POLLUTANTS:\n",
    "    if pollutant not in shap_values_dict or pollutant not in shap_importance:\n",
    "        continue\n",
    "    \n",
    "    top_features = shap_importance[pollutant].head(3)['feature'].tolist()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    for idx, feature in enumerate(top_features):\n",
    "        feature_idx = feature_columns.index(feature)\n",
    "        \n",
    "        shap.dependence_plot(\n",
    "            feature_idx,\n",
    "            shap_values_dict[pollutant],\n",
    "            X_shap,\n",
    "            ax=axes[idx],\n",
    "            show=False\n",
    "        )\n",
    "        axes[idx].set_title(f'{feature}', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle(f'SHAP Dependence Plots - {pollutant} (Top 3 Features)', \n",
    "                fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_PATH, f'shap_dependence_{pollutant}.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ SHAP dependence plots created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb99e71e",
   "metadata": {},
   "source": [
    "## 5.10 Country-Specific SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894090ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze SHAP values by country\n",
    "country_shap_analysis = {}\n",
    "\n",
    "for pollutant in TARGET_POLLUTANTS:\n",
    "    if pollutant not in shap_values_dict:\n",
    "        continue\n",
    "    \n",
    "    # Create DataFrame with SHAP values and country info\n",
    "    shap_df = pd.DataFrame(\n",
    "        shap_values_dict[pollutant],\n",
    "        columns=feature_columns\n",
    "    )\n",
    "    shap_df['country'] = test_sample_data['country'].values\n",
    "    \n",
    "    # Calculate mean SHAP by country for neighbor features\n",
    "    neighbor_cols = [col for col in feature_columns if 'neighbor' in col]\n",
    "    \n",
    "    country_neighbor_shap = shap_df.groupby('country')[neighbor_cols].mean().mean(axis=1)\n",
    "    country_neighbor_shap = country_neighbor_shap.sort_values(ascending=False)\n",
    "    \n",
    "    country_shap_analysis[pollutant] = country_neighbor_shap\n",
    "    \n",
    "    print(f\"\\n{pollutant} - Countries most influenced by neighbors (mean SHAP):\")\n",
    "    print(country_neighbor_shap.head(10))\n",
    "\n",
    "# Visualize country-level neighbor influence\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "colors_pol = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "for idx, pollutant in enumerate(TARGET_POLLUTANTS):\n",
    "    if pollutant in country_shap_analysis:\n",
    "        top_countries = country_shap_analysis[pollutant].head(10)\n",
    "        \n",
    "        axes[idx].barh(range(len(top_countries)), top_countries.values, \n",
    "                      color=colors_pol[idx], alpha=0.8)\n",
    "        axes[idx].set_yticks(range(len(top_countries)))\n",
    "        axes[idx].set_yticklabels(top_countries.index)\n",
    "        axes[idx].invert_yaxis()\n",
    "        axes[idx].set_title(f'{pollutant}\\nNeighbor Influence by Country', \n",
    "                          fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Mean SHAP Value (Neighbor Features)', fontsize=11)\n",
    "        axes[idx].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'country_neighbor_influence.png'), \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Country-specific analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a067c4",
   "metadata": {},
   "source": [
    "## 5.11 Feature Category Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3956ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate SHAP importance by feature category\n",
    "def categorize_feature(feature_name):\n",
    "    \"\"\"Categorize feature based on name\"\"\"\n",
    "    if 'neighbor' in feature_name:\n",
    "        return 'Neighbor'\n",
    "    elif any(x in feature_name for x in ['lag_', 'rolling_', 'ewm_', 'change_']):\n",
    "        return 'Temporal_Lag'\n",
    "    elif any(x in feature_name for x in ['year', 'month', 'day', 'week', 'season', 'sin', 'cos']):\n",
    "        return 'Temporal_Calendar'\n",
    "    elif any(x in feature_name for x in ['latitude', 'longitude']):\n",
    "        return 'Spatial'\n",
    "    elif 'country_' in feature_name:\n",
    "        return 'Country_Identity'\n",
    "    elif any(x in feature_name for x in ['ratio', 'product', 'sum', 'AQI']):\n",
    "        return 'Interaction'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "category_importance = {}\n",
    "\n",
    "for pollutant in TARGET_POLLUTANTS:\n",
    "    if pollutant not in shap_importance:\n",
    "        continue\n",
    "    \n",
    "    shap_df = shap_importance[pollutant].copy()\n",
    "    shap_df['category'] = shap_df['feature'].apply(categorize_feature)\n",
    "    \n",
    "    category_totals = shap_df.groupby('category')['mean_abs_shap'].sum().sort_values(ascending=False)\n",
    "    category_importance[pollutant] = category_totals\n",
    "    \n",
    "    print(f\"\\n{pollutant} - Feature Category Importance:\")\n",
    "    for cat, importance in category_totals.items():\n",
    "        pct = (importance / category_totals.sum()) * 100\n",
    "        print(f\"  {cat:20s}: {importance:.4f} ({pct:.2f}%)\")\n",
    "\n",
    "# Visualize category importance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for idx, pollutant in enumerate(TARGET_POLLUTANTS):\n",
    "    if pollutant in category_importance:\n",
    "        cat_data = category_importance[pollutant]\n",
    "        axes[idx].pie(cat_data.values, labels=cat_data.index, autopct='%1.1f%%',\n",
    "                     startangle=90, textprops={'fontsize': 10})\n",
    "        axes[idx].set_title(f'{pollutant}\\nFeature Category Importance', \n",
    "                          fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'category_importance.png'), \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Feature category importance analyzed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa3d830",
   "metadata": {},
   "source": [
    "## 5.12 Export SHAP Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d2e441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SHAP values for each pollutant\n",
    "for pollutant in TARGET_POLLUTANTS:\n",
    "    if pollutant not in shap_values_dict:\n",
    "        continue\n",
    "    \n",
    "    # Create DataFrame with SHAP values\n",
    "    shap_export = pd.DataFrame(\n",
    "        shap_values_dict[pollutant],\n",
    "        columns=feature_columns\n",
    "    )\n",
    "    \n",
    "    # Add metadata\n",
    "    shap_export['country'] = test_sample_data['country'].values\n",
    "    shap_export['date'] = test_sample_data['date'].values\n",
    "    shap_export['actual_value'] = test_sample_data[pollutant].values\n",
    "    \n",
    "    # Save\n",
    "    shap_export.to_csv(os.path.join(OUTPUT_PATH, f'shap_values_{pollutant}.csv'), index=False)\n",
    "    shap_export.to_pickle(os.path.join(OUTPUT_PATH, f'shap_values_{pollutant}.pkl'))\n",
    "    \n",
    "    print(f\"✓ SHAP values exported for {pollutant}\")\n",
    "\n",
    "print(\"\\n✓ All SHAP results exported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597f75cf",
   "metadata": {},
   "source": [
    "## 5.13 Policy Insights Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b2989",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXPLAINABLE AI - POLICY INSIGHTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. TRANSBOUNDARY INFLUENCE:\")\n",
    "for pollutant in TARGET_POLLUTANTS:\n",
    "    if pollutant in neighbor_influence:\n",
    "        neighbor_pct = neighbor_influence[pollutant]['neighbor_percentage']\n",
    "        print(f\"   {pollutant}: {neighbor_pct:.2f}% influenced by neighboring countries\")\n",
    "\n",
    "print(\"\\n2. KEY DRIVER IDENTIFICATION:\")\n",
    "for pollutant in TARGET_POLLUTANTS:\n",
    "    if pollutant in shap_importance:\n",
    "        top_feature = shap_importance[pollutant].iloc[0]\n",
    "        print(f\"   {pollutant}: Most important feature - {top_feature['feature']}\")\n",
    "\n",
    "print(\"\\n3. MOST AFFECTED COUNTRIES (by neighbor pollution):\")\n",
    "for pollutant in TARGET_POLLUTANTS:\n",
    "    if pollutant in country_shap_analysis:\n",
    "        most_affected = country_shap_analysis[pollutant].idxmax()\n",
    "        print(f\"   {pollutant}: {most_affected}\")\n",
    "\n",
    "print(\"\\n4. FEATURE CATEGORY RANKINGS:\")\n",
    "for pollutant in TARGET_POLLUTANTS:\n",
    "    if pollutant in category_importance:\n",
    "        top_category = category_importance[pollutant].idxmax()\n",
    "        print(f\"   {pollutant}: {top_category} features dominate\")\n",
    "\n",
    "print(\"\\n5. ACTIONABLE INSIGHTS:\")\n",
    "avg_neighbor_influence = np.mean([neighbor_influence[p]['neighbor_percentage'] \n",
    "                                   for p in TARGET_POLLUTANTS if p in neighbor_influence])\n",
    "\n",
    "if avg_neighbor_influence > 30:\n",
    "    print(\"   → Strong transboundary effects detected (>30%)\")\n",
    "    print(\"   → International cooperation is CRITICAL for pollution control\")\n",
    "    print(\"   → Unilateral policies may have limited effectiveness\")\n",
    "else:\n",
    "    print(\"   → Moderate transboundary effects detected\")\n",
    "    print(\"   → Local policies remain primary driver\")\n",
    "    print(\"   → Regional cooperation recommended but not critical\")\n",
    "\n",
    "print(\"\\n6. OUTPUT FILES CREATED:\")\n",
    "print(\"   - shap_summary_<pollutant>.png\")\n",
    "print(\"   - shap_importance_<pollutant>.csv\")\n",
    "print(\"   - neighbor_vs_self_influence.csv\")\n",
    "print(\"   - country_neighbor_influence.png\")\n",
    "print(\"   - shap_values_<pollutant>.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ EXPLAINABLE AI ANALYSIS COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nReady for spatial analysis and GIS visualization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084e3be2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Completed Tasks:\n",
    "1. ✓ Applied SHAP to all best models\n",
    "2. ✓ Extracted global feature importance\n",
    "3. ✓ Quantified self vs neighbor influence\n",
    "4. ✓ Analyzed country-specific transboundary effects\n",
    "5. ✓ Categorized feature importance by type\n",
    "6. ✓ Generated policy-relevant insights\n",
    "7. ✓ Created comprehensive visualizations\n",
    "\n",
    "### Key Findings:\n",
    "- **Transboundary influence** quantified for each pollutant\n",
    "- **Top drivers** identified through SHAP values\n",
    "- **Country vulnerability** mapped by neighbor dependency\n",
    "- **Policy implications** derived from model explanations\n",
    "\n",
    "### Next Steps:\n",
    "**Notebook 06: Spatial Analysis**\n",
    "- Spatial autocorrelation (Moran's I)\n",
    "- Hotspot analysis (Getis-Ord Gi*)\n",
    "- Spatial regression models\n",
    "- Network flow visualization preparation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
